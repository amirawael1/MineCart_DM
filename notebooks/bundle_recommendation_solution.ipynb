{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5eb432",
   "metadata": {},
   "source": [
    "### Bundle & Cross-Selling Recommendation System\n",
    "### Data Mining Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb3913",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk plotly seaborn wordcloud scikit-learn gensim mlxtend --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788c070",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "**Explanation:**  \n",
    "We load the Amazon products dataset and categories from CSV files. The products dataset contains product information including ASIN, title, price, ratings, and category mappings. The categories file provides category names for each category_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# Load products data\n",
    "products_df = pd.read_csv(f\"{DATA_DIR}/amazon_products_cleaned.csv\")\n",
    "print(f\"Loaded {len(products_df)} products\")\n",
    "print(f\"Columns: {products_df.columns.tolist()}\")\n",
    "\n",
    "# Load categories data\n",
    "categories_df = pd.read_csv(f\"{DATA_DIR}/amazon_categories.csv\")\n",
    "print(f\"\\nLoaded {len(categories_df)} categories\")\n",
    "\n",
    "# Merge products with category names\n",
    "df = products_df.merge(categories_df, left_on='category_id', right_on='id', how='left')\n",
    "df = df.drop(columns=['id'])\n",
    "print(f\"\\nMerged dataset shape: {df.shape}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ab3d5",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "**Explanation:**  \n",
    "- Missing values are filled or removed to ensure data quality.\n",
    "- Prices are handled correctly (convert to numeric).\n",
    "- Product titles are cleaned to prepare for text analysis.\n",
    "- We create additional features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text for analysis\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Clean the dataset\n",
    "df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "\n",
    "# Convert types\n",
    "df['stars'] = pd.to_numeric(df['stars'], errors='coerce')\n",
    "df['reviews'] = pd.to_numeric(df['reviews'], errors='coerce').fillna(0).astype(int)\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['listPrice'] = pd.to_numeric(df['listPrice'], errors='coerce')\n",
    "df['boughtInLastMonth'] = pd.to_numeric(df['boughtInLastMonth'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "df['stars'] = df['stars'].fillna(0)\n",
    "df['price'] = df['price'].fillna(df['price'].median())\n",
    "df['category_name'] = df['category_name'].fillna('Unknown')\n",
    "\n",
    "# Calculate discount percentage\n",
    "df['discount_pct'] = np.where(\n",
    "    (df['listPrice'] > 0) & (df['listPrice'] > df['price']),\n",
    "    ((df['listPrice'] - df['price']) / df['listPrice'] * 100).round(2),\n",
    "    0\n",
    ")\n",
    "\n",
    "# Drop rows with missing critical data\n",
    "df.dropna(subset=['asin', 'title', 'category_id'], inplace=True)\n",
    "\n",
    "print(f\"Cleaned dataset: {len(df)} products\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194b6d1",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "**Results Summary:**  \n",
    "- Understand the distribution of products across categories\n",
    "- Analyze price distributions and rating patterns\n",
    "- Identify best-selling products and popular categories\n",
    "- Explore relationships between price, ratings, and sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748b531",
   "metadata": {},
   "source": [
    "## Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "category_counts = df['category_name'].value_counts().head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "category_counts.plot(kind='barh', color='steelblue')\n",
    "plt.title('Top 20 Product Categories by Count')\n",
    "plt.xlabel('Number of Products')\n",
    "plt.ylabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique categories: {df['category_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b41109",
   "metadata": {},
   "source": [
    "## Star Rating Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dff73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter products with valid ratings\n",
    "rated_products = df[df['stars'] > 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(rated_products['stars'], bins=50, kde=True, color='coral')\n",
    "plt.title('Star Rating Distribution')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(rated_products['stars'].mean(), color='red', linestyle='--', label=f\"Mean: {rated_products['stars'].mean():.2f}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average rating: {rated_products['stars'].mean():.2f}\")\n",
    "print(f\"Median rating: {rated_products['stars'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200989d6",
   "metadata": {},
   "source": [
    "## Price Distribution by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution for top categories\n",
    "top_categories = df['category_name'].value_counts().head(10).index.tolist()\n",
    "df_top = df[df['category_name'].isin(top_categories)]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "df_top.boxplot(column='price', by='category_name', vert=False, figsize=(14, 8))\n",
    "plt.title('Price Distribution by Top 10 Categories')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.xlim(0, df_top['price'].quantile(0.95))  # Limit to 95th percentile\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d8464",
   "metadata": {},
   "source": [
    "## Best Sellers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best sellers by category\n",
    "best_sellers = df[df['isBestSeller'] == True]\n",
    "print(f\"Total Best Sellers: {len(best_sellers)}\")\n",
    "\n",
    "best_seller_by_category = best_sellers.groupby('category_name').size().sort_values(ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "best_seller_by_category.plot(kind='bar', color='gold', edgecolor='black')\n",
    "plt.title('Best Sellers by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Best Sellers')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4067a8",
   "metadata": {},
   "source": [
    "## Top Products by Sales Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top products by boughtInLastMonth\n",
    "top_selling = df.nlargest(20, 'boughtInLastMonth')[['title', 'category_name', 'price', 'stars', 'boughtInLastMonth']]\n",
    "\n",
    "print(\"Top 20 Products by Monthly Sales:\")\n",
    "display(top_selling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7e24f",
   "metadata": {},
   "source": [
    "## Price vs Rating Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price vs Rating scatter plot\n",
    "sample_df = df[(df['stars'] > 0) & (df['price'] < df['price'].quantile(0.95))].sample(min(5000, len(df)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(sample_df['price'], sample_df['stars'], alpha=0.3, c='steelblue')\n",
    "plt.title('Price vs Star Rating')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "correlation = df[df['stars'] > 0][['price', 'stars']].corr()\n",
    "print(f\"\\nCorrelation between Price and Rating: {correlation.loc['price', 'stars']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c87b34",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "**Explanation:**  \n",
    "This section creates new features that will be useful for bundle recommendations:\n",
    "- Price buckets for segmentation\n",
    "- Popularity scores based on reviews and sales\n",
    "- Text features from product titles\n",
    "- Category-level statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9039190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Price Buckets\n",
    "df['price_bucket'] = pd.cut(\n",
    "    df['price'],\n",
    "    bins=[0, 10, 25, 50, 100, 250, np.inf],\n",
    "    labels=['<$10', '$10-25', '$25-50', '$50-100', '$100-250', '>$250']\n",
    ")\n",
    "\n",
    "# Popularity Score (normalized combination of reviews and monthly sales)\n",
    "scaler = MinMaxScaler()\n",
    "df['reviews_norm'] = scaler.fit_transform(df[['reviews']])\n",
    "df['sales_norm'] = scaler.fit_transform(df[['boughtInLastMonth']])\n",
    "df['popularity_score'] = 0.5 * df['reviews_norm'] + 0.5 * df['sales_norm']\n",
    "\n",
    "# Rating Quality Score (weighted by number of reviews)\n",
    "df['rating_quality'] = df['stars'] * np.log1p(df['reviews'])\n",
    "\n",
    "# Value Score (rating relative to price)\n",
    "df['value_score'] = df['stars'] / np.log1p(df['price'] + 1)\n",
    "\n",
    "# Category average price and rating\n",
    "category_stats = df.groupby('category_name').agg({\n",
    "    'price': 'mean',\n",
    "    'stars': 'mean',\n",
    "    'boughtInLastMonth': 'sum'\n",
    "}).rename(columns={\n",
    "    'price': 'category_avg_price',\n",
    "    'stars': 'category_avg_rating',\n",
    "    'boughtInLastMonth': 'category_total_sales'\n",
    "})\n",
    "\n",
    "df = df.merge(category_stats, on='category_name', how='left')\n",
    "\n",
    "# Price relative to category average\n",
    "df['price_vs_category'] = df['price'] / df['category_avg_price']\n",
    "\n",
    "print(\"Engineered Features:\")\n",
    "print(df[['title', 'price_bucket', 'popularity_score', 'rating_quality', 'value_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b2768",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Products: {len(df):,}\")\n",
    "print(f\"Unique Categories: {df['category_name'].nunique()}\")\n",
    "print(f\"Best Sellers: {df['isBestSeller'].sum():,}\")\n",
    "print(f\"\\nPrice Statistics:\")\n",
    "print(f\"  Mean: ${df['price'].mean():.2f}\")\n",
    "print(f\"  Median: ${df['price'].median():.2f}\")\n",
    "print(f\"  Range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "print(f\"\\nRating Statistics (rated products only):\")\n",
    "rated = df[df['stars'] > 0]\n",
    "print(f\"  Mean: {rated['stars'].mean():.2f}\")\n",
    "print(f\"  Products with 4+ stars: {(rated['stars'] >= 4).sum():,} ({(rated['stars'] >= 4).mean()*100:.1f}%)\")\n",
    "print(f\"\\nSales Statistics:\")\n",
    "print(f\"  Total Monthly Sales: {df['boughtInLastMonth'].sum():,}\")\n",
    "print(f\"  Products with Sales: {(df['boughtInLastMonth'] > 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673ce93",
   "metadata": {},
   "source": [
    "# Common Words in Product Titles (WordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec268ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add domain-specific stopwords\n",
    "custom_stops = {'pack', 'set', 'pcs', 'piece', 'inch', 'size', 'color', 'new', 'use', 'best', 'free'}\n",
    "stop_words.update(custom_stops)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    tokens = text.split()\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# Collect all words from titles\n",
    "all_words = []\n",
    "for title in df['title'].dropna():\n",
    "    all_words.extend(tokenize(title))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "most_common = word_counts.most_common(30)\n",
    "\n",
    "print(\"Top 30 Words in Product Titles:\")\n",
    "word_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])\n",
    "word_df.index = word_df.index + 1\n",
    "display(word_df)\n",
    "\n",
    "# Generate WordCloud\n",
    "wordcloud = WordCloud(\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    max_words=100\n",
    ").generate_from_frequencies(dict(word_counts))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words in Product Titles', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402ffc1",
   "metadata": {},
   "source": [
    "# Setup (Part 2) - Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load Word2Vec model\n",
    "print(\"Loading Word2Vec model (this may take a few minutes)...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "print(\"Word2Vec model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021cb4ce",
   "metadata": {},
   "source": [
    "# Clustering for Product Grouping\n",
    "**Explanation:**  \n",
    "We use K-Means clustering to group similar products together based on:\n",
    "- TF-IDF features from product titles\n",
    "- Numerical features (price, rating, popularity)\n",
    "\n",
    "These clusters will help identify natural product groupings for bundle recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text for TF-IDF vectorization\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', \"\", text)\n",
    "    text = re.sub(r'\\d+', \"\", text)\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            cleaned_tokens.append(lemma)\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Sample data for clustering (for efficiency)\n",
    "sample_size = min(10000, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "# Preprocess titles\n",
    "df_sample['processed_title'] = df_sample['title'].apply(preprocess_text)\n",
    "df_sample = df_sample[df_sample['processed_title'].str.strip().astype(bool)]\n",
    "\n",
    "print(f\"Clustering on {len(df_sample)} products...\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "tfidf_matrix = tfidf.fit_transform(df_sample['processed_title'])\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "numeric_features = scaler.fit_transform(df_sample[['price', 'stars', 'popularity_score']].fillna(0))\n",
    "\n",
    "# Combine features\n",
    "combined_features = np.hstack([tfidf_array, numeric_features])\n",
    "\n",
    "# Find optimal k using silhouette score\n",
    "silhouette_scores = []\n",
    "k_range = range(3, 12)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(combined_features)\n",
    "    score = silhouette_score(combined_features, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k={k}: Silhouette Score = {score:.4f}\")\n",
    "\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b44a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final clustering with optimal k\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "df_sample['cluster'] = kmeans_final.fit_predict(combined_features)\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(combined_features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=df_sample['cluster'], cmap='tab10', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.title(f'Product Clusters (k={best_k})')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Cluster summary\n",
    "print(\"\\nCluster Summary:\")\n",
    "cluster_summary = df_sample.groupby('cluster').agg({\n",
    "    'price': 'mean',\n",
    "    'stars': 'mean',\n",
    "    'popularity_score': 'mean',\n",
    "    'asin': 'count'\n",
    "}).rename(columns={'asin': 'product_count'}).round(2)\n",
    "display(cluster_summary)\n",
    "\n",
    "# Top categories per cluster\n",
    "print(\"\\nTop Categories per Cluster:\")\n",
    "for cluster_id in sorted(df_sample['cluster'].unique()):\n",
    "    top_cats = df_sample[df_sample['cluster'] == cluster_id]['category_name'].value_counts().head(3)\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(top_cats.index.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b256600",
   "metadata": {},
   "source": [
    "# Content-Based Product Recommendations with Word2Vec\n",
    "**Explanation:**  \n",
    "We implement a content-based recommendation system using Word2Vec embeddings:\n",
    "- Product titles are tokenized and converted to vector representations\n",
    "- Cosine similarity is used to find similar products\n",
    "- This forms the foundation for cross-selling recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize and clean text for Word2Vec\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', \"\", text)\n",
    "    tokens = text.split()\n",
    "    return [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "\n",
    "def get_product_vector(tokens, model):\n",
    "    \"\"\"Convert tokens to averaged Word2Vec vector\"\"\"\n",
    "    valid_tokens = [t for t in tokens if t in model]\n",
    "    if len(valid_tokens) < 2:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[t] for t in valid_tokens], axis=0)\n",
    "\n",
    "# Prepare product data for recommendations\n",
    "rec_df = df[['asin', 'title', 'category_name', 'price', 'stars', 'popularity_score']].copy()\n",
    "rec_df = rec_df.drop_duplicates(subset='asin').reset_index(drop=True)\n",
    "rec_df['tokens'] = rec_df['title'].apply(tokenize_text)\n",
    "rec_df = rec_df[rec_df['tokens'].apply(len) >= 3]  # Keep products with enough tokens\n",
    "\n",
    "print(f\"Preparing vectors for {len(rec_df)} products...\")\n",
    "\n",
    "# Create product vectors\n",
    "product_vectors = np.array([get_product_vector(tokens, w2v_model) for tokens in rec_df['tokens']])\n",
    "\n",
    "# Normalize price and add to feature vector\n",
    "scaler = MinMaxScaler()\n",
    "price_norm = scaler.fit_transform(rec_df[['price']].fillna(0))\n",
    "\n",
    "# Combined feature vector (Word2Vec + normalized price)\n",
    "combined_vectors = np.hstack([product_vectors, price_norm])\n",
    "\n",
    "print(f\"Product vector shape: {combined_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_similar_products(product_idx, n_recommendations=5):\n",
    "    \"\"\"Recommend similar products using cosine similarity\"\"\"\n",
    "    # Calculate similarity with all products\n",
    "    similarities = cosine_similarity([combined_vectors[product_idx]], combined_vectors)[0]\n",
    "    \n",
    "    # Get top similar products (excluding itself)\n",
    "    similar_indices = similarities.argsort()[::-1][1:n_recommendations+1]\n",
    "    \n",
    "    return similar_indices, similarities[similar_indices]\n",
    "\n",
    "# Demo: Recommend for a random product\n",
    "random_idx = random.randint(0, len(rec_df) - 1)\n",
    "selected_product = rec_df.iloc[random_idx]\n",
    "\n",
    "print(f\"Selected Product:\")\n",
    "print(f\"  Title: {selected_product['title'][:100]}...\")\n",
    "print(f\"  Category: {selected_product['category_name']}\")\n",
    "print(f\"  Price: ${selected_product['price']:.2f}\")\n",
    "print(f\"  Rating: {selected_product['stars']}\")\n",
    "\n",
    "similar_idx, scores = recommend_similar_products(random_idx)\n",
    "\n",
    "print(f\"\\nTop 5 Similar Products (Cross-Sell Recommendations):\")\n",
    "print(\"-\" * 80)\n",
    "for i, (idx, score) in enumerate(zip(similar_idx, scores), 1):\n",
    "    product = rec_df.iloc[idx]\n",
    "    print(f\"{i}. {product['title'][:70]}...\")\n",
    "    print(f\"   Category: {product['category_name']} | Price: ${product['price']:.2f} | Similarity: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61f28f",
   "metadata": {},
   "source": [
    "# Association Rule Mining for Bundle Recommendations\n",
    "**Explanation:**  \n",
    "We use the Apriori algorithm to find frequently co-purchased items and generate association rules. This helps identify which products are commonly bought together, enabling bundle recommendations.\n",
    "\n",
    "Since we don't have actual transaction data, we simulate transactions based on category co-occurrence and product similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Simulate transactions based on category groupings and price tiers\n",
    "# Products in the same category and price range are likely to be browsed/bought together\n",
    "\n",
    "def generate_synthetic_transactions(df, n_transactions=5000):\n",
    "    \"\"\"Generate synthetic transactions for association rule mining\"\"\"\n",
    "    transactions = []\n",
    "    \n",
    "    # Group products by category\n",
    "    category_products = df.groupby('category_name')['asin'].apply(list).to_dict()\n",
    "    categories = list(category_products.keys())\n",
    "    \n",
    "    for _ in range(n_transactions):\n",
    "        # Randomly select 2-4 categories\n",
    "        n_categories = random.randint(1, 3)\n",
    "        selected_cats = random.sample(categories, min(n_categories, len(categories)))\n",
    "        \n",
    "        basket = []\n",
    "        for cat in selected_cats:\n",
    "            products = category_products[cat]\n",
    "            # Select 1-3 products from each category\n",
    "            n_products = random.randint(1, min(3, len(products)))\n",
    "            basket.extend(random.sample(products, n_products))\n",
    "        \n",
    "        if len(basket) >= 2:\n",
    "            transactions.append(basket)\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "# Generate transactions\n",
    "print(\"Generating synthetic transactions...\")\n",
    "transactions = generate_synthetic_transactions(df, n_transactions=10000)\n",
    "print(f\"Generated {len(transactions)} transactions\")\n",
    "print(f\"Average basket size: {np.mean([len(t) for t in transactions]):.2f} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeaa8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transactions to one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "transaction_df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "print(f\"Transaction matrix shape: {transaction_df.shape}\")\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "print(\"\\nRunning Apriori algorithm...\")\n",
    "frequent_itemsets = apriori(transaction_df, min_support=0.01, use_colnames=True)\n",
    "print(f\"Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "\n",
    "# Generate association rules\n",
    "if len(frequent_itemsets) > 0:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "    rules = rules.sort_values('lift', ascending=False)\n",
    "    print(f\"Generated {len(rules)} association rules\")\n",
    "    \n",
    "    # Display top rules\n",
    "    print(\"\\nTop 10 Association Rules (by Lift):\")\n",
    "    display(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "else:\n",
    "    print(\"No frequent itemsets found. Try lowering min_support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad542b4",
   "metadata": {},
   "source": [
    "# Category-Based Bundle Recommendations\n",
    "**Explanation:**  \n",
    "We analyze which categories are frequently bought together and create bundle recommendations based on:\n",
    "- Category co-occurrence patterns\n",
    "- Complementary product relationships\n",
    "- Price optimization for bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5410fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze category co-occurrence in transactions\n",
    "def get_category_from_asin(asin, df):\n",
    "    \"\"\"Get category name for a product\"\"\"\n",
    "    match = df[df['asin'] == asin]['category_name']\n",
    "    return match.iloc[0] if len(match) > 0 else None\n",
    "\n",
    "# Create category-level transactions\n",
    "category_transactions = []\n",
    "for transaction in transactions:\n",
    "    cats = set()\n",
    "    for asin in transaction:\n",
    "        cat = get_category_from_asin(asin, df)\n",
    "        if cat:\n",
    "            cats.add(cat)\n",
    "    if len(cats) >= 2:\n",
    "        category_transactions.append(list(cats))\n",
    "\n",
    "print(f\"Category-level transactions: {len(category_transactions)}\")\n",
    "\n",
    "# Category co-occurrence matrix\n",
    "from itertools import combinations\n",
    "\n",
    "category_pairs = Counter()\n",
    "for trans in category_transactions:\n",
    "    for pair in combinations(sorted(trans), 2):\n",
    "        category_pairs[pair] += 1\n",
    "\n",
    "# Top category pairs\n",
    "print(\"\\nTop 15 Category Pairs (frequently bought together):\")\n",
    "top_pairs = category_pairs.most_common(15)\n",
    "pairs_df = pd.DataFrame(top_pairs, columns=['Category Pair', 'Co-occurrence Count'])\n",
    "pairs_df['Category 1'] = pairs_df['Category Pair'].apply(lambda x: x[0])\n",
    "pairs_df['Category 2'] = pairs_df['Category Pair'].apply(lambda x: x[1])\n",
    "display(pairs_df[['Category 1', 'Category 2', 'Co-occurrence Count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category co-occurrence heatmap\n",
    "top_categories = df['category_name'].value_counts().head(15).index.tolist()\n",
    "cooc_matrix = pd.DataFrame(0, index=top_categories, columns=top_categories)\n",
    "\n",
    "for (cat1, cat2), count in category_pairs.items():\n",
    "    if cat1 in top_categories and cat2 in top_categories:\n",
    "        cooc_matrix.loc[cat1, cat2] = count\n",
    "        cooc_matrix.loc[cat2, cat1] = count\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cooc_matrix, annot=True, fmt='d', cmap='YlOrRd', square=True)\n",
    "plt.title('Category Co-occurrence Heatmap')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf11720",
   "metadata": {},
   "source": [
    "# Bundle Generator\n",
    "**Explanation:**  \n",
    "This function creates optimized product bundles based on:\n",
    "1. Product similarity (from Word2Vec embeddings)\n",
    "2. Category diversity (different but complementary categories)\n",
    "3. Price optimization (target bundle price)\n",
    "4. Quality assurance (minimum rating threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44924638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bundle(anchor_product_idx, bundle_size=3, max_price=None, min_rating=3.5):\n",
    "    \"\"\"\n",
    "    Generate a product bundle based on an anchor product.\n",
    "    \n",
    "    Parameters:\n",
    "    - anchor_product_idx: Index of the main product\n",
    "    - bundle_size: Total number of products in bundle (including anchor)\n",
    "    - max_price: Maximum total bundle price\n",
    "    - min_rating: Minimum rating for bundle products\n",
    "    \"\"\"\n",
    "    anchor = rec_df.iloc[anchor_product_idx]\n",
    "    anchor_category = anchor['category_name']\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity([combined_vectors[anchor_product_idx]], combined_vectors)[0]\n",
    "    \n",
    "    # Create candidate products dataframe\n",
    "    candidates = rec_df.copy()\n",
    "    candidates['similarity'] = similarities\n",
    "    \n",
    "    # Filter candidates\n",
    "    candidates = candidates[\n",
    "        (candidates.index != anchor_product_idx) &  # Exclude anchor\n",
    "        (candidates['stars'] >= min_rating) &  # Minimum rating\n",
    "        (candidates['category_name'] != anchor_category)  # Different category for diversity\n",
    "    ]\n",
    "    \n",
    "    # Sort by similarity and select top candidates\n",
    "    candidates = candidates.sort_values('similarity', ascending=False)\n",
    "    \n",
    "    # Build bundle\n",
    "    bundle = [anchor_product_idx]\n",
    "    bundle_price = anchor['price']\n",
    "    used_categories = {anchor_category}\n",
    "    \n",
    "    for idx, row in candidates.iterrows():\n",
    "        if len(bundle) >= bundle_size:\n",
    "            break\n",
    "        \n",
    "        # Check price constraint\n",
    "        if max_price and (bundle_price + row['price']) > max_price:\n",
    "            continue\n",
    "        \n",
    "        # Prefer category diversity\n",
    "        if row['category_name'] in used_categories:\n",
    "            continue\n",
    "        \n",
    "        bundle.append(idx)\n",
    "        bundle_price += row['price']\n",
    "        used_categories.add(row['category_name'])\n",
    "    \n",
    "    return bundle, bundle_price\n",
    "\n",
    "# Demo: Generate bundles for random products\n",
    "print(\"BUNDLE RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _ in range(3):\n",
    "    anchor_idx = random.randint(0, len(rec_df) - 1)\n",
    "    bundle_indices, total_price = generate_bundle(anchor_idx, bundle_size=4, max_price=200)\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Bundle (Total: ${total_price:.2f}):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, idx in enumerate(bundle_indices):\n",
    "        product = rec_df.iloc[idx]\n",
    "        marker = \"â­ ANCHOR\" if i == 0 else f\"  Item {i}\"\n",
    "        print(f\"{marker}: {product['title'][:50]}...\")\n",
    "        print(f\"         Category: {product['category_name']} | ${product['price']:.2f} | â­{product['stars']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0ac21",
   "metadata": {},
   "source": [
    "# Cross-Selling Recommendation Engine\n",
    "**Explanation:**  \n",
    "A comprehensive cross-selling system that combines:\n",
    "1. Content-based similarity (Word2Vec)\n",
    "2. Category affinity scores\n",
    "3. Price compatibility\n",
    "4. Popularity weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSellRecommender:\n",
    "    def __init__(self, products_df, product_vectors):\n",
    "        self.df = products_df\n",
    "        self.vectors = product_vectors\n",
    "        self.category_affinity = self._compute_category_affinity()\n",
    "    \n",
    "    def _compute_category_affinity(self):\n",
    "        \"\"\"Compute category affinity matrix from co-occurrence\"\"\"\n",
    "        categories = self.df['category_name'].unique()\n",
    "        affinity = pd.DataFrame(0.0, index=categories, columns=categories)\n",
    "        \n",
    "        # Use category pair frequencies\n",
    "        for (cat1, cat2), count in category_pairs.items():\n",
    "            if cat1 in categories and cat2 in categories:\n",
    "                affinity.loc[cat1, cat2] = count\n",
    "                affinity.loc[cat2, cat1] = count\n",
    "        \n",
    "        # Normalize\n",
    "        affinity = affinity / (affinity.max().max() + 1e-9)\n",
    "        return affinity\n",
    "    \n",
    "    def recommend(self, product_idx, n_recommendations=5, \n",
    "                  content_weight=0.5, category_weight=0.3, popularity_weight=0.2):\n",
    "        \"\"\"\n",
    "        Generate cross-sell recommendations.\n",
    "        \n",
    "        Parameters:\n",
    "        - product_idx: Index of the source product\n",
    "        - n_recommendations: Number of recommendations\n",
    "        - content_weight: Weight for content similarity\n",
    "        - category_weight: Weight for category affinity\n",
    "        - popularity_weight: Weight for popularity\n",
    "        \"\"\"\n",
    "        source = self.df.iloc[product_idx]\n",
    "        source_category = source['category_name']\n",
    "        \n",
    "        # Content similarity\n",
    "        content_sim = cosine_similarity([self.vectors[product_idx]], self.vectors)[0]\n",
    "        \n",
    "        # Category affinity\n",
    "        category_scores = np.array([\n",
    "            self.category_affinity.loc[source_category, cat] \n",
    "            if cat in self.category_affinity.index and source_category in self.category_affinity.index\n",
    "            else 0\n",
    "            for cat in self.df['category_name']\n",
    "        ])\n",
    "        \n",
    "        # Popularity scores\n",
    "        popularity = self.df['popularity_score'].fillna(0).values\n",
    "        \n",
    "        # Combine scores\n",
    "        final_scores = (\n",
    "            content_weight * content_sim +\n",
    "            category_weight * category_scores +\n",
    "            popularity_weight * popularity\n",
    "        )\n",
    "        \n",
    "        # Exclude same product and same category\n",
    "        final_scores[product_idx] = -1\n",
    "        same_category_mask = self.df['category_name'] == source_category\n",
    "        final_scores[same_category_mask] = final_scores[same_category_mask] * 0.5  # Penalize same category\n",
    "        \n",
    "        # Get top recommendations\n",
    "        top_indices = final_scores.argsort()[::-1][:n_recommendations]\n",
    "        \n",
    "        return top_indices, final_scores[top_indices]\n",
    "\n",
    "# Initialize recommender\n",
    "recommender = CrossSellRecommender(rec_df, combined_vectors)\n",
    "\n",
    "# Demo recommendations\n",
    "print(\"CROSS-SELL RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _ in range(3):\n",
    "    idx = random.randint(0, len(rec_df) - 1)\n",
    "    product = rec_df.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nðŸ›’ Customer viewing: {product['title'][:60]}...\")\n",
    "    print(f\"   Category: {product['category_name']} | Price: ${product['price']:.2f}\")\n",
    "    print(f\"\\n   Customers also bought:\")\n",
    "    \n",
    "    recs, scores = recommender.recommend(idx, n_recommendations=5)\n",
    "    for i, (rec_idx, score) in enumerate(zip(recs, scores), 1):\n",
    "        rec_product = rec_df.iloc[rec_idx]\n",
    "        print(f\"   {i}. {rec_product['title'][:50]}...\")\n",
    "        print(f\"      {rec_product['category_name']} | ${rec_product['price']:.2f} | Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e16e5f",
   "metadata": {},
   "source": [
    "# Bundle Evaluation Metrics\n",
    "**Explanation:**  \n",
    "We evaluate the quality of our bundles using several metrics:\n",
    "- Diversity: How varied are the categories in the bundle?\n",
    "- Coherence: How similar are the products thematically?\n",
    "- Value: Average rating and popularity of bundled products\n",
    "- Price efficiency: Bundle price vs sum of individual prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bundle(bundle_indices):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a product bundle.\n",
    "    \"\"\"\n",
    "    bundle_products = rec_df.iloc[bundle_indices]\n",
    "    \n",
    "    # Diversity: Number of unique categories / bundle size\n",
    "    n_categories = bundle_products['category_name'].nunique()\n",
    "    diversity = n_categories / len(bundle_indices)\n",
    "    \n",
    "    # Coherence: Average pairwise similarity\n",
    "    bundle_vectors = combined_vectors[bundle_indices]\n",
    "    pairwise_sim = cosine_similarity(bundle_vectors)\n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    upper_triangle = pairwise_sim[np.triu_indices(len(bundle_indices), k=1)]\n",
    "    coherence = upper_triangle.mean() if len(upper_triangle) > 0 else 0\n",
    "    \n",
    "    # Value metrics\n",
    "    avg_rating = bundle_products['stars'].mean()\n",
    "    avg_popularity = bundle_products['popularity_score'].mean()\n",
    "    \n",
    "    # Price\n",
    "    total_price = bundle_products['price'].sum()\n",
    "    \n",
    "    return {\n",
    "        'diversity': diversity,\n",
    "        'coherence': coherence,\n",
    "        'avg_rating': avg_rating,\n",
    "        'avg_popularity': avg_popularity,\n",
    "        'total_price': total_price,\n",
    "        'n_products': len(bundle_indices)\n",
    "    }\n",
    "\n",
    "# Evaluate multiple bundles\n",
    "print(\"BUNDLE QUALITY EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation_results = []\n",
    "for i in range(10):\n",
    "    anchor_idx = random.randint(0, len(rec_df) - 1)\n",
    "    bundle_indices, _ = generate_bundle(anchor_idx, bundle_size=4)\n",
    "    \n",
    "    if len(bundle_indices) >= 2:\n",
    "        metrics = evaluate_bundle(bundle_indices)\n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "print(\"\\nBundle Quality Metrics (averaged over 10 bundles):\")\n",
    "print(f\"  Average Diversity: {eval_df['diversity'].mean():.3f}\")\n",
    "print(f\"  Average Coherence: {eval_df['coherence'].mean():.3f}\")\n",
    "print(f\"  Average Rating: {eval_df['avg_rating'].mean():.2f}\")\n",
    "print(f\"  Average Bundle Price: ${eval_df['total_price'].mean():.2f}\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "display(eval_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e182a7",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "## Implemented Techniques:\n",
    "\n",
    "1. **Data Preprocessing & Feature Engineering**\n",
    "   - Cleaned and normalized product data\n",
    "   - Created popularity scores, value scores, and price buckets\n",
    "   - Generated text features from product titles\n",
    "\n",
    "2. **Clustering (K-Means)**\n",
    "   - Grouped similar products using TF-IDF and numerical features\n",
    "   - Used silhouette scores to find optimal cluster count\n",
    "   - Identified natural product groupings\n",
    "\n",
    "3. **Content-Based Recommendations (Word2Vec)**\n",
    "   - Vectorized product titles using Word2Vec embeddings\n",
    "   - Calculated cosine similarity for product matching\n",
    "   - Combined with price features for better recommendations\n",
    "\n",
    "4. **Association Rule Mining (Apriori)**\n",
    "   - Generated synthetic transactions for basket analysis\n",
    "   - Discovered frequent itemsets and association rules\n",
    "   - Identified category co-occurrence patterns\n",
    "\n",
    "5. **Bundle Generator**\n",
    "   - Created optimized product bundles based on:\n",
    "     - Product similarity\n",
    "     - Category diversity\n",
    "     - Price constraints\n",
    "     - Quality thresholds\n",
    "\n",
    "6. **Cross-Selling Recommender**\n",
    "   - Combined multiple signals:\n",
    "     - Content similarity\n",
    "     - Category affinity\n",
    "     - Popularity weighting\n",
    "   - Configurable weights for different business needs\n",
    "\n",
    "## Key Findings:\n",
    "- Product clustering reveals natural groupings based on title semantics and pricing\n",
    "- Category co-occurrence analysis helps identify complementary product categories\n",
    "- Hybrid approaches (combining content + category + popularity) yield better recommendations\n",
    "- Bundle diversity can be balanced with coherence through careful parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fdc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total Products: {len(df):,}\")\n",
    "print(f\"  Categories: {df['category_name'].nunique()}\")\n",
    "print(f\"  Price Range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "print(f\"\\nRecommendation System:\")\n",
    "print(f\"  Products indexed: {len(rec_df):,}\")\n",
    "print(f\"  Feature dimensions: {combined_vectors.shape[1]}\")\n",
    "print(f\"\\nClustering:\")\n",
    "print(f\"  Optimal clusters: {best_k}\")\n",
    "print(f\"  Best silhouette score: {max(silhouette_scores):.4f}\")\n",
    "print(f\"\\nAssociation Rules:\")\n",
    "if 'rules' in dir() and len(rules) > 0:\n",
    "    print(f\"  Total rules generated: {len(rules)}\")\n",
    "    print(f\"  Max lift: {rules['lift'].max():.2f}\")\n",
    "print(f\"\\nBundle Quality (avg over 10 samples):\")\n",
    "print(f\"  Diversity: {eval_df['diversity'].mean():.3f}\")\n",
    "print(f\"  Coherence: {eval_df['coherence'].mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
